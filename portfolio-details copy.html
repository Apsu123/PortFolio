<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <title>Portfolio Details - Eagle’s AI</title>
  <meta content="Computer vision project for detecting and classifying small mechanical parts using deep learning and automated dataset generation." name="description">
  <meta content="computer vision, AI, object detection, YOLOv8, dataset automation, Python, Google Colab" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Fonts -->
  <link href="https://fonts.googleapis.com" rel="preconnect">
  <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&family=Poppins:wght@300;400;600;700&family=Raleway:wght@300;400;600;700&display=swap" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Main CSS File -->
  <link href="assets/css/main.css" rel="stylesheet">
</head>

<body class="portfolio-details-page">

  <header id="header" class="header dark-background d-flex flex-column">
    <i class="header-toggle d-xl-none bi bi-list"></i>

    <div class="profile-img">
      <img src="assets/img/portfolio/Bird.png" alt="Profile image" class="img-fluid rounded-circle">
    </div>

    <a href="index.html" class="logo d-flex align-items-center justify-content-center">
      <h1 class="sitename">Eagle’s AI</h1>
    </a>

    <div class="social-links text-center">
      <a href="https://github.com/Apsu123/Eagles_AI" class="github"><i class="bi bi-github"></i></a>
    </div>

            <nav id="navmenu" class="navmenu">
      <ul>
        <li><a href="index.html#hero" class="active"><i class="bi bi-house navicon"></i>Home</a></li>
        <li><a href="index.html#about"><i class="bi bi-person navicon"></i> About</a></li>
        <li><a href="index.html#portfolio"><i class="bi bi-images navicon"></i> Portfolio</a></li>
      </ul>
    </nav>

  </header>

  <main class="main">

    <!-- Page Title -->
    <div class="page-title dark-background">
      <div class="container d-lg-flex justify-content-between align-items-center">
        <h1 class="mb-2 mb-lg-0">Eagle’s AI</h1>
        <nav class="breadcrumbs">
          <ol>
            <li><a href="index.html">Home</a></li>
            <li class="current">Portfolio Details</li>
          </ol>
        </nav>
      </div>
    </div><!-- End Page Title -->

    <!-- Portfolio Details Section -->
    <section id="portfolio-details" class="portfolio-details section">

      <div class="container" data-aos="fade-up" data-aos-delay="100">

        <div class="row gy-4">

          <div class="col-lg-8">
            <div class="portfolio-details-slider swiper init-swiper">

              <script type="application/json" class="swiper-config">
                {
                  "loop": true,
                  "speed": 600,
                  "slidesPerView": "auto",
                  "pagination": {
                    "el": ".swiper-pagination",
                    "type": "bullets",
                    "clickable": true
                  }
                }
              </script>

              <div class="swiper-wrapper align-items-center">

                <!-- IMAGES -->
                <div class="swiper-slide">
                  <img src="assets/img/portfolio/train.png" alt="First training done in Google Colab">
                  <p class="mt-2 text-center text-muted">First round of training in Google Colab.
The results were depressing. I had to set the minimum confidence threshold that the model accepts ridiculously low. For the dataset used in the first training sessions, I relied on a service called RoboFlow, which has a manual user interface for image labeling and annotation. All the different LEGO pieces were labeled by their serial numbers. The detection was poor because the classes (labels) got mixed up during dataset creation. As a result, the model associated a single class with multiple different shapes.

In addition, the training was done with monochrome images, while in the real world the images are in color. Another reason for the failure was the very small dataset size and the limited number of augmentations. For a task this complex, at least 100 augmented images per class would be needed even for testing purposes. These issues need to be fixed. Still, it felt nice to see that bounding box appear around the correct object—no matter how low the confidence!</p>
                </div>

                <div class="swiper-slide">
                  <img src="assets/img/portfolio/image.png" alt="First detection example">
                  <p class="mt-2 text-center text-muted">The model detects poorly but successfully identifies the propeller piece.</p>
                </div>

                <div class="swiper-slide">
                  <img src="assets/img/portfolio/image copy.png" alt="No detection case">
                  <p class="mt-2 text-center text-muted">Example of a failed detection — the model couldn’t identify any parts.</p>
                </div>

                <div class="swiper-slide">
                  <img src="assets/img/portfolio/image copy 2.png" alt="Roboflow dataset creation">
                  <p class="mt-2 text-center text-muted">Using Roboflow to create and manage datasets before automation with Python.</p>
                </div>

                <div class="swiper-slide">
                  <img src="assets/img/portfolio/image copy 3.png" alt="Class separation test">
                  <p class="mt-2 text-center text-muted">Testing unique serial numbers for each class in dataset labeling.</p>
                </div>

                <div class="swiper-slide">
                  <img src="assets/img/portfolio/image copy 4.png" alt="Dataset problem analysis">
                  <p class="mt-2 text-center text-muted">The model’s low detection accuracy was caused by mixed classes, few augmentations, and differences between training and testing color schemes.</p>
                </div>

                <div class="swiper-slide">
                  <img src="assets/img/portfolio/image copy 5.png" alt="Manual labeling frustration">
                  <p class="mt-2 text-center text-muted">Manual dataset creation proved slow and error-prone — motivating automation.</p>
                </div>

                <div class="swiper-slide">
                  <img src="assets/img/portfolio/image copy 6.png" alt="Single image detection success">
                  <p class="mt-2 text-center text-muted">Improved detection on single, clear images after retraining.</p>
                </div>

                <div class="swiper-slide">
                  <img src="assets/img/portfolio/image copy 8.png" alt="Angle and size challenges">
                  <p class="mt-2 text-center text-muted">Still struggles with different angles and scales, but can detect unseen images — a major step forward.</p>
                </div>

                <div class="swiper-slide">
                  <img src="assets/img/portfolio/image copy 9.png" alt="New approach plan">
                  <p class="mt-2 text-center text-muted">New approach: more images, better augmentation, RGB input, and cleaner class structure.</p>
                </div>

                <div class="swiper-slide">
                  <img src="assets/img/portfolio/image copy 13.png" alt="Automated pipeline progress">
                  <p class="mt-2 text-center text-muted">Predictions improved with automated dataset creation — still struggles with crowded scenes.</p>
                </div>

                <div class="swiper-slide">
                  <img src="assets/img/portfolio/image copy 15.png" alt="Background confusion issue">
                  <p class="mt-2 text-center text-muted">Model learned individual parts well but struggled when backgrounds contained similar pieces.</p>
                </div>

                <div class="swiper-slide">
                  <img src="assets/img/portfolio/image copy 17.png" alt="Switch to YOLOv8s model">
                  <p class="mt-2 text-center text-muted">Switched to YOLOv8s model for better distinction between intricate Lego-like parts.</p>
                </div>

                <div class="swiper-slide">
                  <img src="assets/img/portfolio/image copy 19.png" alt="Real environment detection test">
                  <p class="mt-2 text-center text-muted">Testing detection in real-world environment with similar backgrounds as training data.</p>
                </div>

                <!-- VIDEOS -->
                <div class="swiper-slide">
                  <video controls src="assets/img/portfolio/2025-07-16 10-48-17.mp4"></video>
                  <p class="mt-2 text-center text-muted"> 

Python pipeline for dataset creation. 
I decided to move from manual to automatic dataset creation. This was important because I could now create larger and more diverse datasets using Python’s image processing libraries such as OpenCV, Scikit-Image, and Pillow. The process begins with obtaining the image data. I could have taken the pictures myself and then augmented them in Python. However, I was interested in testing how quickly and how many free LEGO part images I could download from the internet. 

Using DuckDuckGoSearch, I was able to download hundreds of images very quickly by multithreading, essentially increasing the number of “workers” fetching images simultaneously. The remaining functions in the pipeline do exactly what their names suggest, completing the dataset step by step. The final step is generating a YAML config file for training, which includes the file locations and the class list associated with each LEGO part. .</p>
                </div>

                <div class="swiper-slide">
                  <video controls src="assets/img/portfolio/2025-07-25 12-08-34.mp4"></video>
                  <p class="mt-2 text-center text-muted">Detecting my actual LEGOs.
I poured some LEGO pieces onto a white background and had the model detect the ones I wanted to find. As you can see, the model still struggles to detect real LEGO pieces. The detected part 3022 was almost correct—honestly, I might have mistaken it too at first glance. As mentioned earlier, improving real-world performance would require taking photos of the parts from many angles. Even with the many augmentations I applied in Python, the synthetic images still differ greatly from real scenes.

For example, real parts cast shadows, which can confuse the model. Shadows can hide details and distort shapes. YOLOv8 is primarily used for simpler applications like traffic control, where objects such as cars and people are easier to distinguish. A larger model might be needed to capture the fine details of different LEGO parts, which in turn would require more computing power.</p>
                </div>

                <div class="swiper-slide">
                  <video controls src="assets/img/portfolio/BGupdate.mp4"></video>
                  <p class="mt-2 text-center text-muted">Testing annotations. 
This program draws a bounding box based on the coordinates in the image's annotation file. Annotating the dataset takes a long time. Fortunately, I only need to annotate the original images; thanks to a library called Albumentations, augmentations are automatically applied to the new annotation files. This ensures the bounding boxes adjust correctly when the LEGO part is scaled, moved, or rotated. 

Adding background or negative images is also important because the model needs to learn which parts not to focus on. Currently, the background images and white canvases come from the internet. These need to be replaced to make real-world predictions more accurate. I plan to pour a pile of LEGO pieces onto white cardboard and use that as the background, since that’s the environment where I want the parts to be detected. The target part images should also be taken in a real setting. This poses a dilemma for parts I don’t have: to train the model to find the part, I’d need to find it first. </p>
                </div>

                <div class="swiper-slide">
                  <video controls src="assets/img/portfolio/Fg_BG.mp4"></video>
                  <p class="mt-2 text-center text-muted">Inclusion of negative (non-target) images helps the model learn what to ignore.</p>
                </div>

                <div class="swiper-slide">
                  <video controls src="assets/img/portfolio/PredictFromNew.mp4"></video>
                  <p class="mt-2 text-center text-muted">Model predicting specific parts from random, unseen backgrounds.</p>
                </div>

                <div class="swiper-slide">
                  <video controls src="assets/img/portfolio/RealBGP.mp4"></video>
                  <p class="mt-2 text-center text-muted">Predicting new images.
This program detects LEGO parts and classifies them based on their serial numbers. It uses a YOLOv8 model that I trained on my custom LEGO dataset for 150 epochs. The training took quite a long time because I ran out of Colab's free credits and had to switch to my single GPU. I was able to accelerate it somewhat with the CUDA Python library.

The images contain parts that were randomly augmented so they don’t exist in the training data. This still isn’t a real-life scenario, since the images are sourced from the internet. Additionally, the scale of the parts in the training images doesn’t match the scale of those in the backgrounds. Despite this, the results confirmed the model’s ability to learn specific LEGO parts. The confidence scores were high across all detected parts, and the classes were correct. I used confusion matrices and F1-confidence curves to assess the model’s overall performance.</p>
                </div>

                <div class="swiper-slide">
                  <video controls src="assets/img/portfolio/SmallerParts.mp4"></video>
                  <p class="mt-2 text-center text-muted">Predicting "new" images from real background. These are unseen, augmented images that are put to the correct scale with actual LEGOS. They are still not actual LEGOS, though :( </p>
                </div>

              </div>
              <div class="swiper-pagination"></div>
            </div>
          </div>

          <!-- PROJECT DETAILS -->
          <div class="col-lg-4">
            <div class="portfolio-info" data-aos="fade-up" data-aos-delay="200">
              <h3>Project information</h3>
              <ul>
                <li><strong>Category</strong>: Computer Vision / AI Automation</li>
                <li><strong>Client</strong>: Personal Research Project – Eagle’s AI</li>
                <li><strong>Project date</strong>: July, 2025</li>
                <li><strong>Project URL</strong>: <a href="https://github.com/Apsu123/Eagles_AI">https://github.com/Apsu123/Eagles_AI</a></li>
              </ul>
            </div>
            <div class="portfolio-description" data-aos="fade-up" data-aos-delay="300">
              <h2>AI-assisted Part Detection using YOLOv8 and Automated Dataset Generation</h2>
              <p>
 Eagle’s AI is a personal research project exploring computer vision for detecting individual LEGO parts in cluttered scenes. It begins with small, manually labeled datasets and evolves into an automated Python pipeline for downloading, augmenting, and annotating images. Using YOLOv8, the project investigates how dataset quality, label consistency, and background realism impact model performance. Experiments reveal challenges in fine-grained object detection and highlight the gap between synthetic training data and real-world results.
              </p>
            </div>
          </div>

        </div>

      </div>

    </section><!-- /Portfolio Details Section -->

  </main>

  <footer id="footer" class="footer position-relative light-background">
    <div class="container">
      <div class="copyright text-center ">
        <p>© <span>Copyright</span> <strong class="px-1 sitename">Eagle’s AI</strong> <span>All Rights Reserved</span></p>
      </div>
      <div class="credits">
        Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a> • Adapted by <a href="#">Eagle’s AI team</a>
      </div>
    </div>
  </footer>

  <!-- Scroll Top -->
  <a href="#" id="scroll-top" class="scroll-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Preloader -->
  <div id="preloader"></div>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/typed.js/typed.umd.js"></script>
  <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/imagesloaded/imagesloaded.pkgd.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>

  <!-- Main JS File -->
  <script src="assets/js/main.js"></script>

</body>
</html>
